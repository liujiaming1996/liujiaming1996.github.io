<html>

<head>
  <meta content="text/html; charset=utf-8" http-equiv="Content-Type" />
  <title>Jiaming Liu (刘家铭)</title>
  <meta content="Jiaming Liu, 刘家铭, https://liujiaming1996.github.io/" name="keywords" />
  <style media="screen" type="text/css">
    html,
    body,
    div,
    span,
    applet,
    object,
    iframe,
    h1,
    h2,
    h3,
    h4,
    h5,
    h6,
    p,
    blockquote,
    pre,
    a,
    abbr,
    acronym,
    address,
    big,
    cite,
    code,
    del,
    dfn,
    em,
    font,
    img,
    ins,
    kbd,
    q,
    s,
    samp,
    small,
    strike,
    strong,
    sub,
    tt,
    var,
    dl,
    dt,
    dd,
    ol,
    ul,
    li,
    fieldset,
    form,
    label,
    legend,
    table,
    caption,
    tbody,
    tfoot,
    thead,
    tr,
    th,
    td {
      border: 0pt none;
      font-family: Arial, Helvetica, sans-serif;
      font-size: 100%;
      font-style: inherit;
      font-weight: inherit;
      margin: 0pt;
      outline-color: invert;
      outline-style: none;
      outline-width: 0pt;
      padding: 0pt;
      vertical-align: baseline;
    }

    a {
      color: #043d98;
      text-decoration: none;
    }

    a:focus,
    a:hover {
      color: #f09228;
      text-decoration: none;
    }

    a.paper {
      font-weight: bold;
      font-size: 12pt;
    }

    b.paper {
      font-weight: bold;
      font-size: 12pt;
    }

    * {
      margin: 0pt;
      padding: 0pt;
    }

    body {
      position: relative;
      margin: 2em auto 2em auto;
      width: 825px;
      font-family: Open Sans Light, Helvetica, sans-serif;
      font-size: 14px;
      background: #F4F6F6;
    }

    h2 {
      font-family: Lato, Verdana, Helvetica, sans-serif;
      font-size: 15pt;
      font-weight: 700;
    }

    h3 {
      font-family: Lato, Verdana, Helvetica, sans-serif;
      font-size: 16px;
      font-weight: 700;
    }

    strong {
      /* font-family: Lato, Verdana, Helvetica, sans-serif; */
      /* font-size: 13px; */
      font-weight: bold;
    }

    ul {
      /* list-style: circle; */
      list-style: disc;
    }

    img {
      border: none;
    }

    li {
      padding-bottom: 0.5em;
      margin-left: 1.4em;
    }

    alert {
      font-family: Arial, Helvetica, sans-serif;
      /*font-size: 13px;*/
      /* font-weight: bold; */
      color: #FF0000;
    }

    em,
    i {
      font-style: italic;
    }

    div.section {
      clear: both;
      /* margin-bottom: 1.2em; */
      margin-top: 3em;
      /* background: #F4F6F6; */
      background: #FFFFFF;
    }

    div.spanner {
      clear: both;
    }

    div.paper {
      clear: both;
      margin-top: 0.4em;
      margin-bottom: 0.7em;
      border: 0px solid #ddd;
      background: #fff;
      padding: 0.55em .8em .6em .8em;
      border-top-right-radius: 10px;
      border-top-left-radius: 10px;
      border-bottom-left-radius: 10px;
      border-bottom-right-radius: 10px;
      line-height: 140%;
    }

    div.paper2 {
      clear: both;
      margin-top: 0.4em;
      margin-bottom: 0.7em;
      border: 0px solid #ddd;
      background: #fff;
      padding: 0.55em .8em 0.6em .8em;
      border-top-right-radius: 10px;
      border-top-left-radius: 10px;
      border-bottom-left-radius: 10px;
      border-bottom-right-radius: 10px;
      line-height: 140%;
    }

    /* div.paper:hover { */
    /* background: #FFFDEE; */
    /* background-color: #242d36 ; */
    /* } */

    div.paper2:hover {
      background: #FFFDEE;
      /* background-color: #242d36 ; */
    }

    div.bio {
      clear: both;
      margin-top: 0.4em;
      margin-bottom: 0.7em;
      border: 0px solid #ddd;
      background: #fff;
      padding: 0.55em .8em 0.6em .7em;
      border-top-right-radius: 10px;
      border-top-left-radius: 10px;
      border-bottom-left-radius: 10px;
      border-bottom-right-radius: 10px;
      line-height: 135%;
    }

    div.res {
      clear: both;
      margin-top: 0.4em;
      margin-bottom: 0.4em;
      border: 0px solid #ddd;
      background: #fff;
      padding: 0.65em .8em 0.15em .8em;
      border-top-right-radius: 10px;
      border-top-left-radius: 10px;
      border-bottom-left-radius: 10px;
      border-bottom-right-radius: 10px;
      line-height: 130%;
    }
    div.title{
      margin-bottom: 20px;
      border: 1px solid #ddd;
    }
    div.award {
      clear: both;
      margin-top: 0.4em;
      margin-bottom: 0.4em;
      border: 0px solid #ddd;
      background: #fff;
      padding: 0.65em .8em 0.15em .8em;
      border-top-right-radius: 10px;
      border-top-left-radius: 10px;
      border-bottom-left-radius: 10px;
      border-bottom-right-radius: 10px;
      line-height: 130%;
    }

    div.paper div {
      padding-left: 230px;
    }

    img.paper {
      margin-bottom: 0.5em;
      float: left;
      width: 200px;
    }

    span.blurb {
      font-style: italic;
      display: block;
      margin-top: 0.75em;
      margin-bottom: 0.5em;
    }

    pre,
    code {
      font-family: Open Sans Light, Helvetica, sans-serif;
      font-size: 13px;
      margin: 1em 0;
      padding: 0;
    }

    .bot {
      font-size: 14%;
    }

    .ptypej {
      display: inline;
      padding: .0em .2em .05em;
      font-size: 85%;
      font-weight: bold;
      line-height: 1;
      background-color: #5cb85c;
      color: #FFFFFF;
      text-align: center;
      white-space: nowrap;
      vertical-align: baseline;
      margin-right: 6px;
    }

    .ptypec {
      display: inline;
      padding: .0em .2em .05em;
      font-size: 85%;
      font-weight: bold;
      line-height: 1;
      background-color: #428bca;
      color: #FFFFFF;
      text-align: center;
      white-space: nowrap;
      vertical-align: baseline;
      margin-right: 6px;
    }

    .ptypep {
      display: inline;
      padding: .0em .2em .05em;
      font-size: 85%;
      font-weight: bold;
      line-height: 1;
      background-color: #6B6B6B;
      color: #FFFFFF;
      text-align: center;
      white-space: nowrap;
      vertical-align: baseline;
      margin-right: 6px;
    }

    /* navigation */
    #nav {
      /* font-family: 'Lucida Grande', 'Lucida Sans Unicode', 'Lucida Sans',*/
      /* Corbel, Arial, Helvetica, sans-serif; */
      /* font-family: Georgia, Helvetica, sans-serif; */
      /* position: fixed; */
      /* top: 50px; */
      /* left: 860px; */
      /* margin-left: 810px;     1060 */
      /* width: 92px; */
      /* font-size: 14px; */
      list-style-type: none;
      margin: 0;
      padding: 0;
      overflow: hidden;
      background-color: rgb(194, 62, 25);
      position: fixed;
      top: 0;
      width: 100%;
      left: 0;
      height: 3.8%;
      font-size: 16px;
      /* float: right */
    }

    #nav li {
      /* margin-bottom: 1px; */
      float: right;
    }

    ol {
      list-style: none;
    }

    #nav a {
      /* display: block;
    padding: 6px 9px 7px;
    color: #fff;
    background-color: #455A64;
    text-decoration: none; */
      display: block;
      color: white;
      text-align: center;
      padding: 11px 25px;
      text-decoration: none;
    }

    #nav a:hover {
      color: #ffde00;
      /* background-color: #242d36 ; */
    }
  </style>

  <!-- <script type="text/javascript" src="./files/hidebib.js"></script> -->

  <link href="http://fonts.googleapis.com/css?family=Lato:400,700,400italic,700italic" rel="stylesheet"
    type="text/css" />
</head>

<script>
  (function (i, s, o, g, r, a, m) {
    i['GoogleAnalyticsObject'] = r; i[r] = i[r] || function () {
      (i[r].q = i[r].q || []).push(arguments)
    }, i[r].l = 1 * new Date(); a = s.createElement(o),
      m = s.getElementsByTagName(o)[0]; a.async = 1; a.src = g; m.parentNode.insertBefore(a, m)
  })(window, document, 'script', '//www.google-analytics.com/analytics.js', 'ga');
</script>

<script>
  (function (i, s, o, g, r, a, m) {
    i['GoogleAnalyticsObject'] = r; i[r] = i[r] || function () {
      (i[r].q = i[r].q || []).push(arguments)
    }, i[r].l = 1 * new Date(); a = s.createElement(o),
      m = s.getElementsByTagName(o)[0]; a.async = 1; a.src = g; m.parentNode.insertBefore(a, m)
  })(window, document, 'script', '//www.google-analytics.com/analytics.js', 'ga');

  ga('create', 'UA-66888300-1', 'auto');
  ga('send', 'pageview');

</script>

<!-- <script src="./files/main.js"></script> -->

<body style="background-color:#FFFFFF">

  <ol id="nav">
    <li><a href="mailto:liujiaming@bupt.edu.cn" title="Contact">Contact</a></li>
    <li><a href="#pub" title="Papers">Papers</a></li>
    <li><a href="#news" title="News">News</a></li>
    <li><a href="#home" title="Home">Home</a></li>
  </ol>

  <!-- Home section -->
  <a name="home"><br /><br /><br /></a>
  <div 
    style="margin-bottom: 20px; margin-top: 0.2em; border: 1px solid #ddd; background-color: #fff; padding: 1em; height: 420px; ">
    <div style="margin: 0px auto; width: 100%;">
      <img title="Jiaming Liu (刘家铭)" style="float: right; padding-right: 3em; height: 250px;" src="jiaming2.jpg"
        alt="Jiaming Liu" />
      <div style="padding-left: 2em; vertical-align: top; height: 100px;"><span
          style="line-height: 200%; font-size: 24pt;">Liu Jiaming (刘家铭)</span><br />
        <span style="font-size: 16pt;line-height: 34px;"><strong>Bachelor degree </strong></span><br />
          <span style="font-size: 12pt;line-height: 34px;"><a href="https://www.bupt.edu.cn/">北京邮电大学 Beijing University of Posts and Telecommunications</a></span><br />  
          <span style="font-size: 12pt;line-height: 34px;"><a href="https://www.qmul.ac.uk/">伦敦大学玛丽女王学院Queen Mary University of London</a></span><br />
        <span style="font-size: 16pt;line-height: 34px;"><strong>Master graduate </strong></span><br />
          <span style="font-size: 12pt;line-height: 34px;"><a href="https://www.bupt.edu.cn/">北京邮电大学 Beijing University of Posts and Telecommunications</a></span><br />  
          <span style="font-size: 12pt;line-height: 34px;"><a href="https://www.bupt.edu.cn/">Pattern Recognition and Intelligent Systems Lab (PRIS Lab) </a></span><br />
        <span style="font-size: 16pt;line-height: 34px;"><strong>P.h.D candidate </strong></span><br />
          <span style="font-size: 12pt;line-height: 34px;"><a href="https://www.pku.edu.cn/">北京大学 Peking University</a></span><br />
          <span style="font-size: 12pt;line-height: 34px;"><a href="http://idm.pku.edu.cn/">National Engineering Research Center of Visual Technology (NELVT) </a></span><br />
        <span style="font-size: 12pt;line-height: 34px;"><strong>Email: </strong><a href="mailto:">liujiaming@bupt.edu.cn</a>, </strong><a href="mailto:">18610014040@163.com</a></span><br />
        <!-- <span style="font-size: 12pt;line-height: 34px;"><strong>Address: </strong><a>J12/ 1 Cleveland St, Darlington, NSW 2008, Australia</a></span><br /> -->
      </div>
    </div>
  </div>
  <!-- <div style="clear: both; background-color: #fff; margin-top: 1.5em; padding: .2em; padding-left: .3em;"> -->

  <div style="margin-top: 0.2em;">
    <div class="section">
      <h2>About Me (<a href="https://scholar.google.com/citations?hl=zh-CN&user=cPki5sUAAAAJ">Google Scholar</a>)</h2>
      <h3> Supervisor</h3>
      <div class="bio">
        I am currently a P.h.D student at NELVT Lab, Peking University, China, supervised by Prof. <a href="https://scholar.google.com/citations?user=voqw10cAAAAJ&hl=en">Shanghang Zhang</a> .<br>

        <b><font color="Red">
          We have several academic visitor and intern positions at BLV Lab (peking university).
          We actively work on Out Of Distribution Generalization, Automatic Driving Perception, Multi-Modal Learning, and 3D Vision.
          If you like what we do, don't hesitate to contact me.
          </font></b>
      </div>
      <h3> Internship</h3>
      <div class="bio">
        <!-- I am currently a P.h.D Candidate at TML Lab, University of Sydney, Australia, supervised by Prof. <a href="https://tongliang-liu.github.io/">Tongliang Liu</a> and co-supervised by Prof. <a href="https://mingming-gong.github.io/">Mingming Gong</a>.
        I will graduate in 07/2022 and participate in TML Lab as a Ph.D candidate.
        From 01/2020 to 10/2020, I was a research intern at Institute of Deep Learning, Baidu Research, supervised by <a href="https://scholar.google.com/citations?user=f2Y5nygAAAAJ&hl=en">Guodong Guo</a>.
        From 02/2021 to 09/2021, I was a research intern at Y-tech, Kuaishou Technology, supervised by <a href="https://sites.google.com/site/utsqiangli2/">Qiang Li</a>.
        Now, I am a research intern at OPPO Research Institute, supervised by <a href="https://scholar.google.com/citations?user=fWDoWsQAAAAJ&hl=en">Yandong Guo</a>. -->
        
        From 06/2020 to 05/2021, I was a research intern at Intel Label China, supervised by Ming lu. <br>
        From 05/2021 to 08/2021, I was a algorithm engineering intern at Alibaba AI Lab, supervised by Gen Li.<br>
        From 08/2021 to now, I am a research intern at OPPO Research Institute, supervised by <a href="https://scholar.google.com/citations?user=fWDoWsQAAAAJ&hl=en">Yandong Guo</a>.<br>
<!--         
        <b><font color="Red">
        We have several intern positions at OPPO Research Institute.
        We actively work on Computer Vision, 3D Graphics, and multi-modal learning.
        If you like what we do, don't hesitate to contact me.
        </font></b> -->
      </div>

    </div>
  </div>

  <div style="clear: both;">
    <div class="section">
      <a name="interests"></a>
      <h2>Research Interests</h2>
      <div class="bio">
        <ul>
          My research mainly focuses on Automatic Driving Generalization, Multi-Modal, and Neural Video Delivrey.
        </ul>

      </div>
    </div>
  </div>

  <!-- News -->
  <a name="news"></a>
  <div style="clear: both;">
    <div class="section">
      <h2>News</h2>
      <div class="paper">
        <ul>
          <!-- <li> <alert>I will graduate in 2020 fall and I am now looking for a Ph.D. program. Please feel free to contact me.</alert> </li> -->
          <li> 2023: We (UniOCC) achieved 3rd place in the <b>CVPR2023 Challenge - Vision-Centric 3D Occupancy Prediction</b>.</li>
          <li> 2023: Three papers were accepted by <b>MICCAI2023</b>, <b>ICASSP2023</b>, and <b>NOSSDAV2023</b>.</li>
          <li> 2023: Three papers were accepted by <b>CVPR2023</b> (Autonomous Driving + Quantization).</li>
          <li> 2022: Three papers were accepted by <b>ECCV2022</b> (One Oral + Two Posters).</li>
          <li> 2021: One paper were accepted by <b>BMVC2021</b> (Patch Sampling Augmentation).</li>
          <li> 2021: One paper was accepted by <b>ICCV2021</b> (Neural Video Delivery).</li>
          <li> 2019: One paper was accepted by <b>IEEE Access</b> (Satellite Image Semantic segmentation).</li>
        </ul>
      </div>
    </div>
  </div>

  <a name="pub"></a>
  <div style="clear: both;">
    <div class="section">
      <h2 id="confpapers">Publications</h2>

      <!-- ECCV2022 -->
      <div class="paper" id="ECCV2022_3"><img class="paper" src="papers/ECCV2022/ECCV2022_3.png" />
        <div>
          <p><a><b>MTTrans: Cross-Domain Object Detection with Mean Teacher Transformer</b></a><br />
            Jinze Yu, <u><b>Jiaming Liu</b></u>, Xiaobao Wei, Haoyi Zhou, Yohei Nakata, Denis Gudovskiy, Tomoyuki Okuno, Jianxin Li, Kurt Keutzer, Shanghang Zhang
          </p>European Conference on Computer Vision (<b><font color="DarkRed">ECCV</font></b>), 2022 <br /><p>
            <!-- <b><font color="Green">[Poster]</font></b>&nbsp; -->
            <a href='https://arxiv.org/abs/2205.01643'>[PDF]</a>&nbsp;
            <!-- <a href='https://github.com/Neural-video-delivery/EMT-Pytorch-ECCV2022'>[Code]</a>&nbsp; -->
            <!-- <a href='papers/CVPRW2020/CVPR20_EDLCV_Oral_15min.pdf'>[Slides]</a>&nbsp; -->
        </div>
        <div class="spanner"></div>
      </div>

      <div class="paper" id="ECCV2022"><img class="paper" src="papers/ECCV2022/ECCV2022_2.png" />
        <div>
          <p><a><b>Adaptive Patch Exiting for Scalable Single Image Super-Resolution</b></a><br />
            Shizun Wang,  <u><b>Jiaming Liu (equal contribution)</b></u>, Kaixin Chen, Xiaoqi Li, Ming Lu, Yandong Guo
          </p>European Conference on Computer Vision (<b><font color="DarkRed">ECCV</font></b>), 2022 <br /><p>
          <b><font color="Green">[Oral]</font></b>&nbsp;<br>
            <a href='https://arxiv.org/abs/2203.11589'>[PDF]</a>&nbsp;
            <a href='https://github.com/littlepure2333/APE'>[Code]</a>&nbsp;
            <!-- <a href='papers/CVPRW2020/CVPR20_EDLCV_Oral_15min.pdf'>[Slides]</a>&nbsp; -->
        </div>
        <div class="spanner"></div>
      </div>

      <div class="paper" id="ECCV2022"><img class="paper" src="papers/ECCV2022/ECCV2022_1.png" />
        <div>
          <p><a><b>Efficient Meta-Tuning for Content-aware Neural Video Delivery</b></a><br />
            Xiaoqi Li, <u><b>Jiaming Liu (equal contribution)</b></u>, Shizun Wang, Cheng Lyu, Ming Lu, Yurong Chen, Anbang Yao, Yandong Guo, Shanghang Zhang
          </p>European Conference on Computer Vision (<b><font color="DarkRed">ECCV</font></b>), 2022 <br /><p>
            <!-- <b><font color="Green">[Poster]</font></b>&nbsp; -->
            <a href='https://arxiv.org/abs/2207.09691'>[PDF]</a>&nbsp;
            <a href='https://github.com/Neural-video-delivery/EMT-Pytorch-ECCV2022'>[Code]</a>&nbsp;
            <!-- <a href='papers/CVPRW2020/CVPR20_EDLCV_Oral_15min.pdf'>[Slides]</a>&nbsp; -->
        </div>
        <div class="spanner"></div>
      </div>
      
      <!-- BMVC2021 -->
      <div class="paper" id="bmvc2021"><img class="paper" src="papers/BMVC2021/BMVC2021.png" />
        <div>
          <p><a><b>SamplingAug: On the Importance of Patch Sampling Augmentation for Single Image Super-Resolution</b></a><br />
            Shizun Wang, Ming Lu, Kaixin Chen, <u><b>Jiaming Liu</b></u>, Xiaoqi Li, Ming Wu
          </p>The British Machine Vision Conference (<b><font color="DarkRed">BMVC</font></b>), 2021 <br /><p>
            <!-- <b><font color="Green">[Poster]</font></b>&nbsp; -->
            <a href='https://arxiv.org/abs/2111.15185'>[PDF]</a>&nbsp;
            <a href='https://github.com/littlepure2333/SamplingAug'>[Code]</a>&nbsp;
            <!-- <a href='papers/CVPRW2020/CVPR20_EDLCV_Oral_15min.pdf'>[Slides]</a>&nbsp; -->
        </div>
        <div class="spanner"></div>
      </div>

       <!-- ICCV2021 -->
       <div class="paper" id="ICCV2021"><img class="paper" src="papers/ICCV2021/ICCV2021.png" />
        <div>
          <p><a><b>Overfitting the Data: Compact Neural Video Delivery via Content-aware Feature Modulation</b></a><br />
            <u><b>Jiaming Liu</b></u>, Ming Lu, Kaixin Chen, Xiaoqi Li, Shizun Wang, Zhaoqing Wang, Enhua Wu, Yurong Chen, Chuang Zhang, Ming Wu
          </p>International Conference on Computer Vision (<b><font color="DarkRed">ICCV</font></b>), 2021 <br /><p>
            <!-- <b><font color="Green">[Poster]</font></b>&nbsp; -->
            <a href='https://openaccess.thecvf.com/content/ICCV2021/papers/Liu_Overfitting_the_Data_Compact_Neural_Video_Delivery_via_Content-Aware_Feature_ICCV_2021_paper.pdf'>[PDF]</a>&nbsp;
            <a href='https://github.com/Neural-video-delivery/CaFM-Pytorch-ICCV2021'>[Code]</a>&nbsp;
            <!-- <a href='papers/CVPRW2020/CVPR20_EDLCV_Oral_15min.pdf'>[Slides]</a>&nbsp; -->
        </div>
        <div class="spanner"></div>
      </div>

      <div class="paper" id="access2019"><img class="paper" src="papers/ACCESS2019/ACCESS2019.PNG" />
        <div>
          <p><a><b>Towards Accurate High Resolution Satellite Image Semantic Segmentation</b></a><br />
            Ming Wu, Chuang Zhang, <u><b>Jiaming Liu</b></u>, Lichen Zhou, Xiaoqi Li
          </p>IEEE ACCESS, 2019 <br /><p>
            <!-- <b><font color="Green">[Poster]</font></b>&nbsp; -->
            <a href='https://ieeexplore.ieee.org/abstract/document/8700168'>[PDF]</a>&nbsp;
            <!-- <a href='https://github.com/Junjun2016/APCNet'>[Code]</a>&nbsp; -->
            <!-- <a href='papers/CVPRW2020/CVPR20_EDLCV_Oral_15min.pdf'>[Slides]</a>&nbsp; -->
        </div>
        <div class="spanner"></div>
      </div>

     

      <!-- IGARSS2021 -->
      <!-- <div class="paper" id="IGARSS2021"><img class="paper" src="papers/IGARSS2021/vecnet.png" />
        <div>
          <p><a><b>Vecnet: A Spectral and Multi-Scale Spatial Fusion Deep Network for Pixel-Level Cloud Type Classification in Himawari-8 Imagery</b></a><br />
            <u><b>Zhaoqing Wang</b></u>, Xiangyu Kong, Zhanbei Cui, Ming Wu, Chuang Zhang, MingMing Gong, Tongliang Liu
          </p>International Geoscience and Remote Sensing Symposium (<b><font color="DarkRed">IGARSS</font></b>), 2021 <br /><p>
            <b><font color="Green">[Poster]</font></b>&nbsp;
            <a href='https://ieeexplore.ieee.org/abstract/document/9554737'>[PDF]</a>&nbsp;
            <a href='papers/CVPRW2020/CVPR20_EDLCV_Oral_15min.pdf'>[Slides]</a>&nbsp;
        </div>
        <div class="spanner"></div>
      </div> --> 

      <!-- <div style="clear: both;">
        <div class="section">
          <h2>Professional Services</h2>
          <div class="paper">
            <li> Reviewer for CVPR, ICCV, ICML, IGARSS, and ICPR</li>
          </div>
        </div>
      </div> -->

      <div style="clear:both;">
        <p align="right">
          <font size="5">Last Updated on 29th August, 2022</a></font>
        </p>
      </div>

      <!-- <hr>
      <div id="clustrmaps-widget"></div>
      <script type="text/javascript" id="clustrmaps"
        src="//cdn.clustrmaps.com/map_v2.js?cl=ffffff&w=300&t=tt&d=3SbcvEtmc7QY9AEk0QS8aX5NjWpb6rfNMsOGTchiXzs"></script> -->

</body>

</html>