<html>

<head>
  <meta content="text/html; charset=utf-8" http-equiv="Content-Type" />
  <title>Jiaming Liu (刘家铭)</title>
  <meta content="Jiaming Liu, 刘家铭, https://liujiaming1996.github.io/" name="keywords" />
  <style media="screen" type="text/css">
    html,
    body,
    div,
    span,
    applet,
    object,
    iframe,
    h1,
    h2,
    h3,
    h4,
    h5,
    h6,
    p,
    blockquote,
    pre,
    a,
    abbr,
    acronym,
    address,
    big,
    cite,
    code,
    del,
    dfn,
    em,
    font,
    img,
    ins,
    kbd,
    q,
    s,
    samp,
    small,
    strike,
    strong,
    sub,
    tt,
    var,
    dl,
    dt,
    dd,
    ol,
    ul,
    li,
    fieldset,
    form,
    label,
    legend,
    table,
    caption,
    tbody,
    tfoot,
    thead,
    tr,
    th,
    td {
      border: 0pt none;
      font-family: Arial, Helvetica, sans-serif;
      font-size: 100%;
      font-style: inherit;
      font-weight: inherit;
      margin: 0pt;
      outline-color: invert;
      outline-style: none;
      outline-width: 0pt;
      padding: 0pt;
      vertical-align: baseline;
    }

    a {
      color: #043d98;
      text-decoration: none;
    }

    a:focus,
    a:hover {
      color: #f09228;
      text-decoration: none;
    }

    a.paper {
      font-weight: bold;
      font-size: 12pt;
    }

    b.paper {
      font-weight: bold;
      font-size: 12pt;
    }

    * {
      margin: 0pt;
      padding: 0pt;
    }

    body {
      position: relative;
      margin: 2em auto 2em auto;
      width: 825px;
      font-family: Open Sans Light, Helvetica, sans-serif;
      font-size: 14px;
      background: #F4F6F6;
    }

    h2 {
      font-family: Lato, Verdana, Helvetica, sans-serif;
      font-size: 15pt;
      font-weight: 700;
    }

    h3 {
      font-family: Lato, Verdana, Helvetica, sans-serif;
      font-size: 16px;
      font-weight: 700;
    }

    strong {
      /* font-family: Lato, Verdana, Helvetica, sans-serif; */
      /* font-size: 13px; */
      font-weight: bold;
    }

    ul {
      /* list-style: circle; */
      list-style: disc;
    }

    img {
      border: none;
    }

    li {
      padding-bottom: 0.5em;
      margin-left: 1.4em;
    }

    alert {
      font-family: Arial, Helvetica, sans-serif;
      /*font-size: 13px;*/
      /* font-weight: bold; */
      color: #FF0000;
    }

    em,
    i {
      font-style: italic;
    }

    div.section {
      clear: both;
      /* margin-bottom: 1.2em; */
      margin-top: 3em;
      /* background: #F4F6F6; */
      background: #FFFFFF;
    }

    div.spanner {
      clear: both;
    }

    div.paper {
      clear: both;
      margin-top: 0.4em;
      margin-bottom: 0.7em;
      border: 0px solid #ddd;
      background: #fff;
      padding: 0.55em .8em .6em .8em;
      border-top-right-radius: 10px;
      border-top-left-radius: 10px;
      border-bottom-left-radius: 10px;
      border-bottom-right-radius: 10px;
      line-height: 140%;
    }

    div.paper2 {
      clear: both;
      margin-top: 0.4em;
      margin-bottom: 0.7em;
      border: 0px solid #ddd;
      background: #fff;
      padding: 0.55em .8em 0.6em .8em;
      border-top-right-radius: 10px;
      border-top-left-radius: 10px;
      border-bottom-left-radius: 10px;
      border-bottom-right-radius: 10px;
      line-height: 140%;
    }

    /* div.paper:hover { */
    /* background: #FFFDEE; */
    /* background-color: #242d36 ; */
    /* } */

    div.paper2:hover {
      background: #FFFDEE;
      /* background-color: #242d36 ; */
    }

    div.bio {
      clear: both;
      margin-top: 0.4em;
      margin-bottom: 0.7em;
      border: 0px solid #ddd;
      background: #fff;
      padding: 0.55em .8em 0.6em .7em;
      border-top-right-radius: 10px;
      border-top-left-radius: 10px;
      border-bottom-left-radius: 10px;
      border-bottom-right-radius: 10px;
      line-height: 135%;
    }

    div.res {
      clear: both;
      margin-top: 0.4em;
      margin-bottom: 0.4em;
      border: 0px solid #ddd;
      background: #fff;
      padding: 0.65em .8em 0.15em .8em;
      border-top-right-radius: 10px;
      border-top-left-radius: 10px;
      border-bottom-left-radius: 10px;
      border-bottom-right-radius: 10px;
      line-height: 130%;
    }
    div.title{
      margin-bottom: 20px;
      border: 1px solid #ddd;
    }
    div.award {
      clear: both;
      margin-top: 0.4em;
      margin-bottom: 0.4em;
      border: 0px solid #ddd;
      background: #fff;
      padding: 0.65em .8em 0.15em .8em;
      border-top-right-radius: 10px;
      border-top-left-radius: 10px;
      border-bottom-left-radius: 10px;
      border-bottom-right-radius: 10px;
      line-height: 130%;
    }

    div.paper div {
      padding-left: 230px;
    }

    img.paper {
      margin-bottom: 0.5em;
      float: left;
      width: 200px;
    }

    span.blurb {
      font-style: italic;
      display: block;
      margin-top: 0.75em;
      margin-bottom: 0.5em;
    }

    pre,
    code {
      font-family: Open Sans Light, Helvetica, sans-serif;
      font-size: 13px;
      margin: 1em 0;
      padding: 0;
    }

    .bot {
      font-size: 14%;
    }

    .ptypej {
      display: inline;
      padding: .0em .2em .05em;
      font-size: 85%;
      font-weight: bold;
      line-height: 1;
      background-color: #5cb85c;
      color: #FFFFFF;
      text-align: center;
      white-space: nowrap;
      vertical-align: baseline;
      margin-right: 6px;
    }

    .ptypec {
      display: inline;
      padding: .0em .2em .05em;
      font-size: 85%;
      font-weight: bold;
      line-height: 1;
      background-color: #428bca;
      color: #FFFFFF;
      text-align: center;
      white-space: nowrap;
      vertical-align: baseline;
      margin-right: 6px;
    }

    .ptypep {
      display: inline;
      padding: .0em .2em .05em;
      font-size: 85%;
      font-weight: bold;
      line-height: 1;
      background-color: #6B6B6B;
      color: #FFFFFF;
      text-align: center;
      white-space: nowrap;
      vertical-align: baseline;
      margin-right: 6px;
    }

    /* navigation */
    #nav {
      /* font-family: 'Lucida Grande', 'Lucida Sans Unicode', 'Lucida Sans',*/
      /* Corbel, Arial, Helvetica, sans-serif; */
      /* font-family: Georgia, Helvetica, sans-serif; */
      /* position: fixed; */
      /* top: 50px; */
      /* left: 860px; */
      /* margin-left: 810px;     1060 */
      /* width: 92px; */
      /* font-size: 14px; */
      list-style-type: none;
      margin: 0;
      padding: 0;
      overflow: hidden;
      background-color: rgb(194, 62, 25);
      position: fixed;
      top: 0;
      width: 100%;
      left: 0;
      height: 3.8%;
      font-size: 16px;
      /* float: right */
    }

    #nav li {
      /* margin-bottom: 1px; */
      float: right;
    }

    ol {
      list-style: none;
    }

    #nav a {
      /* display: block;
    padding: 6px 9px 7px;
    color: #fff;
    background-color: #455A64;
    text-decoration: none; */
      display: block;
      color: white;
      text-align: center;
      padding: 11px 25px;
      text-decoration: none;
    }

    #nav a:hover {
      color: #ffde00;
      /* background-color: #242d36 ; */
    }
  </style>

  <!-- <script type="text/javascript" src="./files/hidebib.js"></script> -->

  <link href="http://fonts.googleapis.com/css?family=Lato:400,700,400italic,700italic" rel="stylesheet"
    type="text/css" />
</head>

<script>
  (function (i, s, o, g, r, a, m) {
    i['GoogleAnalyticsObject'] = r; i[r] = i[r] || function () {
      (i[r].q = i[r].q || []).push(arguments)
    }, i[r].l = 1 * new Date(); a = s.createElement(o),
      m = s.getElementsByTagName(o)[0]; a.async = 1; a.src = g; m.parentNode.insertBefore(a, m)
  })(window, document, 'script', '//www.google-analytics.com/analytics.js', 'ga');
</script>

<script>
  (function (i, s, o, g, r, a, m) {
    i['GoogleAnalyticsObject'] = r; i[r] = i[r] || function () {
      (i[r].q = i[r].q || []).push(arguments)
    }, i[r].l = 1 * new Date(); a = s.createElement(o),
      m = s.getElementsByTagName(o)[0]; a.async = 1; a.src = g; m.parentNode.insertBefore(a, m)
  })(window, document, 'script', '//www.google-analytics.com/analytics.js', 'ga');

  ga('create', 'UA-66888300-1', 'auto');
  ga('send', 'pageview');

</script>

<!-- <script src="./files/main.js"></script> -->

<body style="background-color:#FFFFFF">

  <ol id="nav">
    <li><a href="mailto:liujiaming@bupt.edu.cn" title="Contact">Contact</a></li>
    <li><a href="#pub" title="Papers">Papers</a></li>
    <li><a href="#news" title="News">News</a></li>
    <li><a href="#home" title="Home">Home</a></li>
  </ol>

  <!-- Home section -->
  <a name="home"><br /><br /><br /></a>
  <div 
    style="margin-bottom: 20px; margin-top: 0.2em; border: 1px solid #ddd; background-color: #fff; padding: 1em; height: 500px; ">
    <div style="margin: 0px auto; width: 100%;">
      <img title="Jiaming Liu (刘家铭)" style="float: right; padding-right: 3em; height: 250px;" src="jiaming2.jpg"
        alt="Jiaming Liu" />
      <div style="padding-left: 2em; vertical-align: top; height: 100px;"><span
          style="line-height: 200%; font-size: 24pt;">Liu Jiaming (刘家铭)</span><br />
        <span style="font-size: 16pt;line-height: 34px;"><strong>Bachelor degree (2015-2019) </strong></span><br />
          <span style="font-size: 12pt;line-height: 34px;"><a href="https://www.bupt.edu.cn/">北京邮电大学 Beijing University of Posts and Telecommunications</a></span><br />  
          <span style="font-size: 12pt;line-height: 34px;"><a href="https://www.qmul.ac.uk/">伦敦大学玛丽女王学院Queen Mary University of London</a></span><br />
        <span style="font-size: 16pt;line-height: 34px;"><strong>Master graduate (2019-2022) </strong></span><br />
          <span style="font-size: 12pt;line-height: 34px;"><a href="https://www.bupt.edu.cn/">北京邮电大学 Beijing University of Posts and Telecommunications</a></span><br />  
          <span style="font-size: 12pt;line-height: 34px;"><a href="https://www.bupt.edu.cn/">Pattern Recognition and Intelligent Systems Lab (PRIS Lab) </a></span><br />
        <span style="font-size: 16pt;line-height: 34px;"><strong>Employee (2022-2023)</strong></span><br />
          <span style="font-size: 12pt;line-height: 34px;"><a href="https://www.idea.edu.cn/team/5829.html">OPPO研究院 OPPO Research Institute </a></span><br />  
          <span style="font-size: 12pt;line-height: 34px;"><a href="https://www.idea.edu.cn/team/5829.html">Supervised by OPPO Chief Scientist - Yandong Guo </a></span><br />
        <span style="font-size: 16pt;line-height: 34px;"><strong>P.h.D candidate (2023-Now)</strong></span><br />
          <span style="font-size: 12pt;line-height: 34px;"><a href="https://www.pku.edu.cn/">北京大学 Peking University, School of Computer Science</a></span><br />
          <span style="font-size: 12pt;line-height: 34px;"><a href="http://idm.pku.edu.cn/">National Key Laboratory for Multimedia Information Processing</a></span><br />
        <span style="font-size: 12pt;line-height: 34px;"><strong>Email: </strong><a href="mailto:">liujiaming@bupt.cn</a>, </strong><a href="mailto:">jiamingliu@stu.pku.edu.cn</a></span><br />
        <!-- <span style="font-size: 12pt;line-height: 34px;"><strong>Address: </strong><a>J12/ 1 Cleveland St, Darlington, NSW 2008, Australia</a></span><br /> -->
      </div>
    </div>
  </div>
  <!-- <div style="clear: both; background-color: #fff; margin-top: 1.5em; padding: .2em; padding-left: .3em;"> -->

  <div style="margin-top: 0.2em;">
    <div class="section">
      <h2>About Me (<a href="https://scholar.google.com/citations?hl=zh-CN&user=cPki5sUAAAAJ">Google Scholar</a>)</h2>
      <h3> Supervisor</h3>
      <div class="bio">
        I am currently a P.h.D student at State Key Laboratory for Multimedia Information Processing, School of Computer Science, Peking University, China, supervised by Prof. <a href="https://scholar.google.com/citations?user=voqw10cAAAAJ&hl=en">Shanghang Zhang</a> .<br>

        <b><font color="Red">
          We have several academic visitor and intern positions at HMI Lab (Peking University).
          We actively work on Robotics, Multi-Modal Learning, and 3D Vision.
          If you like what we do, don't hesitate to contact me.
          </font></b>
      </div>
      <h3>Research Interests</h3>
      <div class="bio">
        <ul>      
        My past research mainly focuses on Autonomous Driving, Out Of Distribution, and Neural Video Delivery. My current research direction is Robotic manipulation, Continual learning, and Multimodal Large Langugae Model.
        </ul>
      </div>
      <!-- <h3> Internship</h3>
      <div class="bio">
        From 06/2020 to 05/2021, I was a research intern at Intel Label China, supervised by Ming lu. <br>
        From 05/2021 to 08/2021, I was a algorithm engineering intern at Alibaba AI Lab, supervised by Gen Li.<br>
        From 08/2021 to 03/2023, I was a research intern at OPPO Research Institute, supervised by <a href="https://scholar.google.com/citations?user=fWDoWsQAAAAJ&hl=en">Yandong Guo</a>.<br>
      </div> -->

    </div>
  </div>

  <!-- <div style="clear: both;">
    <div class="section">
      <a name="interests"></a>
      <h2>Research Interests</h2>
      <div class="bio">
        <ul>      
        My past research mainly focuses on Autonomous Driving, Out Of Distribution, and Neural Video Delivery. My current research direction is Continual learning, Embodied AI, and 3D vision.
        </ul>

      </div>
    </div>
  </div> -->

  <!-- News -->
  <a name="news"></a>
  <div style="clear: both;">
    <div class="section">
      <h2>News</h2>
      <div class="paper">
        <ul>
          <li> 2025: Three papers were accepted by <b>CVPR2025</b> (3D Robotic manipulation + Vision-Language-Action Model).</li>
          <li> 2025: One papers was accepted by <b>AAAI2025</b> (Multimodal Large Language Models + Autonomous Driving).</li>
          <li> 2025: Two papers were accepted by <b>ICLR2025</b> (Multimodal Large Language Models + Math).</li>
          <li> 2024: One paper was accepted by <b>Neurips2024</b> (Robotic manipulation + Multimodal Large Language Models).</li>
          <li> 2024: Two papers were accepted by <b>ECCV2024</b> (Multimodal Large Language Models + 3D large-scale model).</li>
          <!-- <li> 2024: Six papers were accepted by <b>CVPR2024</b> (Continual learning + Multimodal Large Language Models + Robotic Manipulation).</li>
          <li> 2024: Four papers were accepted by <b>ICRA2024</b> (BEV 3D detection + 3D Occupancy prediction + Continual learning + Spike camera).</li>
          <li> 2024: One paper was accepted by <b>ICLR2024</b> (Parameter efficient fine-tuning + Continual learning).</li> -->
          <!-- <li> 2024: Three papers were accepted by <b>AAAI2024</b> (BEV 3D detection + MOE + Continual learning).</li> -->
          <!-- <li> 2023: We achieved two champions in the <b>ICCV2023 Challenge - Multitask Learning &  Continual Test-time Adaptation</b>.</li>
          <li> 2023: We (UniOCC) achieved 3rd place in the <b>CVPR2023 Challenge - Vision-Centric 3D Occupancy Prediction</b>.</li>
          <li> 2023: Three papers were accepted by <b>MICCAI2023</b>, <b>ICASSP2023</b>, and <b>NOSSDAV2023</b>.</li>
          <li> 2023: Three papers were accepted by <b>CVPR2023</b> (BEV 3D detection + Continual Learning + Quantization).</li> -->
          <!-- <li> 2022: Three papers were accepted by <b>ECCV2022</b> (One Oral + Two Posters).</li> -->
          <!-- <li> 2021: One paper were accepted by <b>BMVC2021</b> (Patch Sampling Augmentation).</li> -->
          <!-- <li> 2021: One paper was accepted by <b>ICCV2021</b> (Neural Video Delivery).</li> -->
          <!-- <li> 2019: One paper was accepted by <b>IEEE Access</b> (Satellite Image Semantic segmentation).</li> -->
        </ul>
      </div>
    </div>
  </div>

  <a name="pub"></a>
  <div style="clear: both;">
    <div class="section">
      <h2 id="confpapers">Publications (First author or Project leader)</h2>

      <!-- pre 2025 -->
      <div class="paper" id="pre_1"><img class="paper" src="papers/images/HybridVLA.png" />
        <div>
          <p><a><b>HybridVLA: Collaborative Diffusion and Autoregression in a Unified Vision-Language-Action Model</b></a><br />
            <u><b>Jiaming Liu</b></u>, Hao Chen, Pengju An, Zhuoyang Liu, Renrui Zhang, Chenyang Gu, Xiaoqi Li, ..., Pheng-Ann Heng, Shanghang Zhang
          </p>Pre-print(<b><font color="DarkRed">Arxiv</font></b>), 2025 <br /><p></p>
            <!-- <b><font color="Green">[Poster]</font></b>&nbsp; -->
            <a href='https://arxiv.org/pdf/2503.10631'>[PDF]</a>&nbsp;
            <a href='https://hybrid-vla.github.io/'>[Web page]</a>&nbsp;
            <a href='https://github.com/PKU-HMI-Lab/Hybrid-VLA'>[Code]</a>&nbsp;
        </div>
        <div class="spanner"></div>
      </div>

      <!-- pre 2025 -->
      <div class="paper" id="pre_0"><img class="paper" src="papers/images/robomind.png" />
        <div>
          <p><a><b>Robomind: Benchmark on multi-embodiment intelligence normative data for robot manipulation</b></a><br />
            Kun Wu, Chengkai Hou, <u><b>Jiaming Liu (equal contribution)</b></u>, Zhengping Che, Xiaozhu Ju, ..., Shanghang Zhang, Jian Tang
          </p>Pre-print(<b><font color="DarkRed">Arxiv</font></b>), 2025 <br /><p></p>
            <!-- <b><font color="Green">[Poster]</font></b>&nbsp; -->
            <a href='https://arxiv.org/pdf/2412.13877'>[PDF]</a>&nbsp;
            <a href='https://x-humanoid-robomind.github.io/'>[Web page]</a>&nbsp;
        </div>
        <div class="spanner"></div>
      </div>

      <!-- CVPR 2025 -->
      <div class="paper" id="cvpr25_0"><img class="paper" src="papers/images/lift3D.png" />
        <div>
          <p><a><b>Lift3d foundation policy: Lifting 2d large-scale pretrained models for robust 3d robotic manipulation</b></a><br />
            Yueru Jia, <u><b>Jiaming Liu (equal contribution)</b></u>, Sixiang Chen, Chenyang Gu, Zhilue Wang, Longzan Luo, Lily Lee, Pengwei Wang, Zhongyuan Wang, Renrui Zhang, Shanghang Zhang
          </p>Computer Vision and Pattern Recognition (<b><font color="DarkRed">CVPR</font></b>), 2025 <br /><p></p>
            <!-- <b><font color="Green">[Poster]</font></b>&nbsp; -->
            <a href='https://arxiv.org/pdf/2411.18623'>[PDF]</a>&nbsp;
            <a href='https://lift3d-web.github.io/'>[Web page]</a>&nbsp;
            <a href='https://github.com/PKU-HMI-Lab/LIFT3D'>[Code]</a>&nbsp;
        </div>
        <div class="spanner"></div>
      </div>

      <!-- AAAI 2025 -->
      <div class="paper" id="aaai25_0"><img class="paper" src="papers/images/lidarllm.png" />
        <div>
          <p><a><b>Lidar-llm: Exploring the potential of large language models for 3d lidar understanding</b></a><br />
            Senqiao Yang, <u><b>Jiaming Liu (equal contribution)</b></u>, Ray Zhang, Mingjie Pan, Zoey Guo, Xiaoqi Li, Zehui Chen, Peng Gao, Yandong Guo, Shanghang Zhang
          </p>Thirty-ninth AAAI Conference on Artificial Intelligence (<b><font color="DarkRed">AAAI</font></b>), 2025 <br /><p></p>
            <!-- <b><font color="Green">[Poster]</font></b>&nbsp; -->
            <a href='https://arxiv.org/pdf/2312.14074'>[PDF]</a>&nbsp;
            <!-- <a href='https://sites.google.com/view/robomamba-web'>[Web page]</a>&nbsp; -->
            <a href='https://github.com/Yangsenqiao/LiDAR-LLM'>[Code]</a>&nbsp;
            <!-- <a href='papers/CVPRW2020/CVPR20_EDLCV_Oral_15min.pdf'>[Slides]</a>&nbsp; -->
        </div>
        <div class="spanner"></div>
      </div>

      <!-- Neurips 2024 -->
      <div class="paper" id="neurips24_0"><img class="paper" src="papers/images/robomamba.png" />
        <div>
          <p><a><b>Robomamba: Efficient vision-language-action model for robotic reasoning and manipulation</b></a><br />
            <u><b>Jiaming Liu</b></u>, Mengzhen Liu, Zhenyu Wang, Pengju An, Xiaoqi Li, Kaichen Zhou, Senqiao Yang, Renrui Zhang, Yandong Guo, Shanghang Zhang
          </p>Advances in Neural Information Processing Systems (<b><font color="DarkRed">Neurips</font></b>), 2024 <br /><p></p>
            <!-- <b><font color="Green">[Poster]</font></b>&nbsp; -->
            <a href='https://proceedings.neurips.cc/paper_files/paper/2024/file/46a126492ea6fb87410e55a58df2e189-Paper-Conference.pdf'>[PDF]</a>&nbsp;
            <a href='https://sites.google.com/view/robomamba-web'>[Web page]</a>&nbsp;
            <a href='https://github.com/lmzpai/roboMamba'>[Code(test)]</a>&nbsp;
            <!-- <a href='papers/CVPRW2020/CVPR20_EDLCV_Oral_15min.pdf'>[Slides]</a>&nbsp; -->
        </div>
        <div class="spanner"></div>
      </div>

      <!-- ECCV 2024 -->
      <div class="paper" id="eccv24_0"><img class="paper" src="papers/ECCV2024/any2point.png" />
        <div>
          <p><a><b>Any2Point: Empowering Any-modality Large Models for Efficient 3D Understanding</b></a><br />
            Yiwen Tang, <u><b>Jiaming Liu (equal contribution)</b></u>, Dong Wang, Zhigang Wang, Shanghang Zhang, Bin Zhao, Xuelong Li
          </p>European Conference on Computer Vision (<b><font color="DarkRed">ECCV</font></b>), 2024 <br /><p></p>
            <!-- <b><font color="Green">[Poster]</font></b>&nbsp; -->
            <a href='https://arxiv.org/pdf/2404.07989'>[PDF]</a>&nbsp;
            <a href='https://github.com/Ivan-Tang-3D/Any2Point'>[Code]</a>&nbsp;
            <!-- <a href='papers/CVPRW2020/CVPR20_EDLCV_Oral_15min.pdf'>[Slides]</a>&nbsp; -->
        </div>
        <div class="spanner"></div>
      </div>

      <!-- CVPR 2024 -->
      <div class="paper" id="cvpr24_0"><img class="paper" src="papers/CVPR2024/continual-mae.png" />
        <div>
          <p><a><b>Continual-MAE: Adaptive Distribution Masked Autoencoders for Continual Test-Time Adaptation</b></a><br />
            <u><b>Jiaming Liu</b></u>, Ran Xu, Senqiao Yang, Renrui Zhang, Qizhe Zhang, Zehui Chen, Yandong Guo, Shanghang Zhang
          </p>Computer Vision and Pattern Recognition (<b><font color="DarkRed">CVPR</font></b>), 2024 <br /><p></p>
            <!-- <b><font color="Green">[Poster]</font></b>&nbsp; -->
            <a href='https://openaccess.thecvf.com/content/CVPR2024/papers/Liu_Continual-MAE_Adaptive_Distribution_Masked_Autoencoders_for_Continual_Test-Time_Adaptation_CVPR_2024_paper.pdf'>[PDF]</a>&nbsp;
            <a href='https://sites.google.com/view/continual-mae/home'>[Web page]</a>&nbsp;
            <!-- <a href='papers/CVPRW2020/CVPR20_EDLCV_Oral_15min.pdf'>[Slides]</a>&nbsp; -->
        </div>
        <div class="spanner"></div>
      </div>

      <!-- CVPR 2024 -->
      <div class="paper" id="cvpr24_1"><img class="paper" src="papers/CVPR2024/CDC.png" />
        <div>
          <p><a><b>Cloud-Device Collaborative Learning for Multimodal Large Language Models</b></a><br />
            Guanqun Wang, <u><b>Jiaming Liu (equal contribution)</b></u>, Chenxuan Li, Yuan Zhang, Junpeng Ma, Xinyu Wei, Kevin Zhang, Maurice Chong, Renrui Zhang, Yijiang Liu, Shanghang Zhang
          </p>Computer Vision and Pattern Recognition (<b><font color="DarkRed">CVPR</font></b>), 2024 <br /><p></p>
            <!-- <b><font color="Green">[Poster]</font></b>&nbsp; -->
            <a href='https://openaccess.thecvf.com/content/CVPR2024/papers/Wang_Cloud-Device_Collaborative_Learning_for_Multimodal_Large_Language_Models_CVPR_2024_paper.pdf'>[PDF]</a>&nbsp;
            <!-- <a href='https://sites.google.com/view/continual-mae/home'>[Web page]</a>&nbsp; -->
            <!-- <a href='papers/CVPRW2020/CVPR20_EDLCV_Oral_15min.pdf'>[Slides]</a>&nbsp; -->
        </div>
        <div class="spanner"></div>
      </div>

      <!-- ICRA 2024 -->
      <div class="paper" id="ICRA_1"><img class="paper" src="papers/preprint/SPIKE.png" />
        <div>
          <p><a><b>Unsupervised Spike Depth Estimation via Cross-modality Cross-domain Knowledge Transfer</b></a><br />
            <u><b>Jiaming Liu</b></u>, Qizhe Zhang, Xiaoqi Li Jianing Li, Ming Lu, Tiejun Huang, Shanghang Zhang
          </p> International Conference on Robotics and Automation <b><font color="DarkRed">ICRA</font></b>, 2024 <br /><p></p>
            <!-- <b><font color="Green">[Poster]</font></b>&nbsp; -->
            <a href='https://arxiv.org/pdf/2208.12527.pdf'>[PDF]</a>&nbsp;
            <a href='https://github.com/Theia-4869/BiCross'>[Code]</a>&nbsp;
            <!-- <a href='papers/CVPRW2020/CVPR20_EDLCV_Oral_15min.pdf'>[Slides]</a>&nbsp; -->
        </div>
        <div class="spanner"></div>
      </div>
      
      <!-- ICRA 2024 -->
      <div class="paper" id="ICRA_0"><img class="paper" src="papers/ICRA2024/BEVUDA.png" />
        <div>
          <p><a><b>Multi-latent Space Alignments for Unsupervised Domain Adaptation in Multi-view 3D Object Detection</b></a><br />
            <u><b>Jiaming Liu</b></u>, Rongyu Zhang, Xiaowei Chi, Xiaoqi Li, Ming Lu, Yandong Guo, Shanghang Zhang
          </p> International Conference on Robotics and Automation <b><font color="DarkRed">ICRA</font></b>, 2024 <br /><p></p>
            <!-- <b><font color="Green">[Poster]</font></b>&nbsp; -->
            <a href='https://arxiv.org/pdf/2211.17126.pdf'>[PDF]</a>&nbsp;
            <a href='https://github.com/liujiaming1996/BEVUDA'>[Code]</a>&nbsp;
            <!-- <a href='papers/CVPRW2020/CVPR20_EDLCV_Oral_15min.pdf'>[Slides]</a>&nbsp; -->
        </div>
        <div class="spanner"></div>
      </div>

      <!-- ICRA 2024 -->
      <div class="paper" id="ICRA_3"><img class="paper" src="papers/ICRA2024/RenderOCC.png" />
        <div>
          <p><a><b>Renderocc: Vision-centric 3d occupancy prediction with 2d rendering supervision</b></a><br />
            Mingjie Pan, <u><b>Jiaming Liu (equal contribution)</b></u>, Renrui Zhang, Peixiang Huang, Xiaoqi Li, Li Liu, Shanghang Zhang 
          </p> International Conference on Robotics and Automation <b><font color="DarkRed">ICRA</font></b>, 2024 <br /><p></p>
            <!-- <b><font color="Green">[Poster]</font></b>&nbsp; -->
            <a href='https://arxiv.org/pdf/2309.09502.pdf'>[PDF]</a>&nbsp;
            <a href='https://github.com/pmj110119/RenderOcc'>[Code]</a>&nbsp;
            <!-- <a href='papers/CVPRW2020/CVPR20_EDLCV_Oral_15min.pdf'>[Slides]</a>&nbsp; -->
        </div>
        <div class="spanner"></div>
      </div>
      
      <!-- ICRA 2024 -->
      <div class="paper" id="ICRA_2"><img class="paper" src="papers/ICRA2024/DAT.png" />
        <div>
          <p><a><b>Distribution-Aware Continual Test Time Adaptation for Semantic Segmentation</b></a><br />
            Jiayi Ni, Senqiao Yang, <u><b>Jiaming Liu (project leader)</b></u>, Xiaoqi Li, Wenyu Jiao, Ran Xu, Zehui Chen, Yi Liu, Shanghang Zhang 
          </p> International Conference on Robotics and Automation <b><font color="DarkRed">ICRA</font></b>, 2024 <br /><p></p>
            <!-- <b><font color="Green">[Poster]</font></b>&nbsp; -->
            <a href='https://arxiv.org/pdf/2309.13604.pdf'>[PDF]</a>&nbsp;
            <!-- <a href='https://github.com/Neural-video-delivery/EMT-Pytorch-ECCV2022'>[Code]</a>&nbsp; -->
            <!-- <a href='papers/CVPRW2020/CVPR20_EDLCV_Oral_15min.pdf'>[Slides]</a>&nbsp; -->
        </div>
        <div class="spanner"></div>
      </div>

      
      <!-- ICLR 2024 -->
      <div class="paper" id="ICLR_1"><img class="paper" src="papers/preprint/ViDA.png" />
        <div>
          <p><a><b>ViDA: Homeostatic Visual Domain Adapter for Continual Test Time Adaptation</b></a><br />
            <u><b>Jiaming Liu</b></u>, Senqiao Yang, Peidong Jia, Ming Lu, Yandong Guo, Wei Xue, Shanghang Zhang
          </p> International Conference on Learning Representations <b><font color="DarkRed">ICLR</font></b>, 2024 <br /><p></p>
            <!-- <b><font color="Green">[Poster]</font></b>&nbsp; -->
            <a href='https://arxiv.org/pdf/2306.04344'>[PDF]</a>&nbsp;
            <a href='https://github.com/Yangsenqiao/vida'>[Code]</a>&nbsp;
            <!-- <a href='papers/CVPRW2020/CVPR20_EDLCV_Oral_15min.pdf'>[Slides]</a>&nbsp; -->
        </div>
        <div class="spanner"></div>
      </div>

      <!-- AAAI 2024 -->
      <div class="paper" id="AAAI_1"><img class="paper" src="papers/preprint/SVDP.png" />
        <div>
          <p><a><b>Exploring Sparse Visual Prompt for Domain Adaptive Dense Prediction</b></a><br />
            Senqiao Yang, Jiarui Wu, <u><b>Jiaming Liu (project leader)</b></u>, Xiaoqi Li, Qizhe Zhang, Mingjie Pan, Shanghang Zhang
          </p> Thirty-Eighth AAAI Conference on Artificial Intelligence <b><font color="DarkRed">AAAI</font></b>, 2024 <br /><p>
            <!-- <b><font color="Green">[Poster]</font></b>&nbsp; -->
            <a href='https://arxiv.org/pdf/2303.09792'>[PDF]</a>&nbsp;
            <a href='https://github.com/Anonymous-012/SVDP'>[Code]</a>&nbsp;
            <!-- <a href='papers/CVPRW2020/CVPR20_EDLCV_Oral_15min.pdf'>[Slides]</a>&nbsp; -->
        </div>
        <div class="spanner"></div>
      </div>

      <!-- NOSSDAV2023 -->
      <div class="paper" id="NOSSDAV2023_1"><img class="paper" src="papers/NOSSDAV2023/REP.png" />
        <div>
          <p><a><b>RepCaM: Re-parameterization Content-aware Modulation for Neural Video Delivery</b></a><br />
            Rongyu Zhang, Lixuan Du, <u><b>Jiaming Liu (project leader)</b></u>, Xiaoqi Li, Ming Lu, Yandong Guo, Shanghang Zhang
          </p>Network and Operating System Support for Digital Audio and Video (<b><font color="DarkRed">NOSSDAV</font></b>), CCF B <br /><p></p>
            <!-- <b><font color="Green">[Poster]</font></b>&nbsp; -->
            <a href='https://dl.acm.org/doi/abs/10.1145/3592473.3592567'>[PDF]</a>&nbsp;
            <!-- <a href='https://opendrivelab.com/AD23Challenge.html#3d_occupancy_prediction'>[Challenge Web]</a>&nbsp; -->
            <!-- <a href='papers/CVPRW2020/CVPR20_EDLCV_Oral_15min.pdf'>[Slides]</a>&nbsp; -->
        </div>
        <div class="spanner"></div>
      </div>

      <div class="paper" id="CVPR2023_3"><img class="paper" src="papers/CVPR2023/BEVSAN.png" />
        <div>
          <p><a><b>BEV-SAN: Accurate BEV 3D Object Detection via Slice Attention Networks</b></a><br />
            Xiaowei Chi, <u><b>Jiaming Liu (equal contribution)</b></u>, Ming Lu, Rongyu Zhang, Zhaoqing Wang, Yandong Guo, Shanghang Zhang
          </p>Computer Vision and Pattern Recognition (<b><font color="DarkRed">CVPR</font></b>), 2023 <br /><p>
            <!-- <b><font color="Green">[Poster]</font></b>&nbsp; -->
            <a href='https://openaccess.thecvf.com/content/CVPR2023/html/Chi_BEV-SAN_Accurate_BEV_3D_Object_Detection_via_Slice_Attention_Networks_CVPR_2023_paper.html'>[PDF]</a>&nbsp;
            <!-- <a href='https://github.com/Neural-video-delivery/EMT-Pytorch-ECCV2022'>[Code]</a>&nbsp; -->
            <!-- <a href='papers/CVPRW2020/CVPR20_EDLCV_Oral_15min.pdf'>[Slides]</a>&nbsp; -->
        </div>
        <div class="spanner"></div>
      </div>
      

      <div class="paper" id="ECCV2022"><img class="paper" src="papers/ECCV2022/ECCV2022_2.png" />
        <div>
          <p><a><b>Adaptive Patch Exiting for Scalable Single Image Super-Resolution</b></a><br />
            Shizun Wang,  <u><b>Jiaming Liu (equal contribution)</b></u>, Kaixin Chen, Xiaoqi Li, Ming Lu, Yandong Guo
          </p>European Conference on Computer Vision (<b><font color="DarkRed">ECCV Oral</font></b>), 2022 <br /><p>
          <!-- <b><font color="Green">[Oral]</font></b>&nbsp;<br> -->
            <a href='https://arxiv.org/abs/2203.11589'>[PDF]</a>&nbsp;
            <a href='https://github.com/littlepure2333/APE'>[Code]</a>&nbsp;
            <!-- <a href='papers/CVPRW2020/CVPR20_EDLCV_Oral_15min.pdf'>[Slides]</a>&nbsp; -->
        </div>
        <div class="spanner"></div>
      </div>

      <div class="paper" id="ECCV2022"><img class="paper" src="papers/ECCV2022/ECCV2022_1.png" />
        <div>
          <p><a><b>Efficient Meta-Tuning for Content-aware Neural Video Delivery</b></a><br />
            Xiaoqi Li, <u><b>Jiaming Liu (equal contribution)</b></u>, Shizun Wang, Cheng Lyu, Ming Lu, Yurong Chen, Anbang Yao, Yandong Guo, Shanghang Zhang
          </p>European Conference on Computer Vision (<b><font color="DarkRed">ECCV</font></b>), 2022 <br /><p>
            <!-- <b><font color="Green">[Poster]</font></b>&nbsp; -->
            <a href='https://arxiv.org/abs/2207.09691'>[PDF]</a>&nbsp;
            <a href='https://github.com/Neural-video-delivery/EMT-Pytorch-ECCV2022'>[Code]</a>&nbsp;
            <!-- <a href='papers/CVPRW2020/CVPR20_EDLCV_Oral_15min.pdf'>[Slides]</a>&nbsp; -->
        </div>
        <div class="spanner"></div>
      </div>

       <!-- ICCV2021 -->
       <div class="paper" id="ICCV2021"><img class="paper" src="papers/ICCV2021/ICCV2021.png" />
        <div>
          <p><a><b>Overfitting the Data: Compact Neural Video Delivery via Content-aware Feature Modulation</b></a><br />
            <u><b>Jiaming Liu</b></u>, Ming Lu, Kaixin Chen, Xiaoqi Li, Shizun Wang, Zhaoqing Wang, Enhua Wu, Yurong Chen, Chuang Zhang, Ming Wu
          </p>International Conference on Computer Vision (<b><font color="DarkRed">ICCV</font></b>), 2021 <br /><p>
            <!-- <b><font color="Green">[Poster]</font></b>&nbsp; -->
            <a href='https://openaccess.thecvf.com/content/ICCV2021/papers/Liu_Overfitting_the_Data_Compact_Neural_Video_Delivery_via_Content-Aware_Feature_ICCV_2021_paper.pdf'>[PDF]</a>&nbsp;
            <a href='https://github.com/Neural-video-delivery/CaFM-Pytorch-ICCV2021'>[Code]</a>&nbsp;
            <!-- <a href='papers/CVPRW2020/CVPR20_EDLCV_Oral_15min.pdf'>[Slides]</a>&nbsp; -->
        </div>
        <div class="spanner"></div>
      </div>

      <!-- <div class="paper" id="access2019"><img class="paper" src="papers/ACCESS2019/ACCESS2019.PNG" />
        <div>
          <p><a><b>Towards Accurate High Resolution Satellite Image Semantic Segmentation</b></a><br />
            Ming Wu, Chuang Zhang, <u><b>Jiaming Liu (first student)</b></u>, Lichen Zhou, Xiaoqi Li
          </p>IEEE ACCESS, 2019 <br /><p>
            <b><font color="Green">[Poster]</font></b>&nbsp;
            <a href='https://ieeexplore.ieee.org/abstract/document/8700168'>[PDF]</a>&nbsp;-->


  <a name="pubs"></a>
  <div style="clear: both;">
    <div class="section">
      <h2 id="confpapers">Publications (Coauthor)</h2>


      <!-- CVPR 2025 -->
      <div class="paper" id="cvpr25_2"><img class="paper" src="papers/images/crayon.png" />
        <div>
          <p><a><b>Object-Centric Prompt-Driven Vision-Language-Action Model for Robotic Manipulation</b></a><br />
            Xiaoqi Li, Lingyun Xu, Mingxu Zhang, <u><b>Jiaming Liu</b></u>, Yan Shen, Iaroslav Ponomarenko, Jiahui Xu, Liang Heng, Siyuan Huang, Shanghang Zhang, Hao Dong
          </p>Computer Vision and Pattern Recognition (<b><font color="DarkRed">CVPR</font></b>), 2025 <br /><p></p>
            <!-- <b><font color="Green">[Poster]</font></b>&nbsp; -->
            <a href='https://openreview.net/pdf?id=Aqfwhna1D7'>[PDF]</a>&nbsp;
            <a href='https://sites.google.com/view/crayonrobo'>[Web page]</a>&nbsp;
        </div>
        <div class="spanner"></div>
      </div>

      <!-- CVPR 2025 -->
      <div class="paper" id="cvpr25_1"><img class="paper" src="papers/images/robobrain.png" />
        <div>
          <p><a><b>RoboBrain: A Unified Brain Model for Robotic Manipulation from Abstract to Concrete</b></a><br />
            Yuheng Ji, Huajie Tan, Jiayu Shi, Xiaoshuai Hao, Yuan Zhang, Hengyuan Zhang, Pengwei Wang, Mengdi Zhao, ..., <u><b>Jiaming Liu</b></u>, Zhongyuan Wang, Shanghang Zhang
          </p>Computer Vision and Pattern Recognition (<b><font color="DarkRed">CVPR</font></b>), 2025 <br /><p></p>
            <!-- <b><font color="Green">[Poster]</font></b>&nbsp; -->
            <a href='https://arxiv.org/pdf/2502.21257'>[PDF]</a>&nbsp;
            <a href='https://superrobobrain.github.io/'>[Web page]</a>&nbsp;
        </div>
        <div class="spanner"></div>
      </div>

      <!-- ICLR 2025 -->
      <div class="paper" id="iclr25_0"><img class="paper" src="papers/images/mathmllm.png" />
        <div>
          <p><a><b>Mavis: Mathematical visual instruction tuning with an automatic data engine</b></a><br />
            Renrui Zhang, Xinyu Wei, Dongzhi Jiang, Ziyu Guo, Shicheng Li, Yichi Zhang, Chengzhuo Tong, <u><b>Jiaming Liu</b></u>, ..., Shanghang Zhang, Peng Gao, Chunyuan Li, Hongsheng Li
          </p> International Conference on Learning Representations <b><font color="DarkRed">ICLR</font></b>, 2025 <br /><p></p>
            <!-- <b><font color="Green">[Poster]</font></b>&nbsp; -->
            <a href='https://arxiv.org/pdf/2407.08739'>[PDF]</a>&nbsp;
            <a href='https://github.com/ZrrSkywalker/MAVIS'>[Code]</a>&nbsp;
        </div>
        <div class="spanner"></div>
      </div>

      <!-- ECCV 2024 -->
      <div class="paper" id="eccv24_2"><img class="paper" src="papers/ECCV2024/LDA.png" />
        <div>
          <p><a><b>LLM as Dataset Analyst: Subpopulation Structure Discovery with Large Language Model</b></a><br />
            Yulin Luo, Ruichuan An, Bocheng Zou, Yiming Tang, <u><b>Jiaming Liu</b></u>, Shanghang Zhang
          </p>European Conference on Computer Vision (<b><font color="DarkRed">ECCV</font></b>), 2024 <br /><p></p>
            <!-- <b><font color="Green">[Poster]</font></b>&nbsp; -->
            <a href='https://arxiv.org/pdf/2405.02363'>[PDF]</a>&nbsp;
            <!-- <a href='https://github.com/Ivan-Tang-3D/Any2Point'>[Code]</a>&nbsp; -->
            <!-- <a href='papers/CVPRW2020/CVPR20_EDLCV_Oral_15min.pdf'>[Slides]</a>&nbsp; -->
        </div>
        <div class="spanner"></div>
      </div>

      <!-- CVPR 2024 -->
      <div class="paper" id="cvpr24_2"><img class="paper" src="papers/CVPR2024/manipllm.png" />
        <div>
          <p><a><b>Manipllm: Embodied multimodal large language model for object-centric robotic manipulation</b></a><br />
            Xiaoqi Li, Mingxu Zhang, Yiran Geng, Haoran Geng, Yuxing Long, Yan Shen, Renrui Zhang, <u><b>Jiaming Liu</b></u>, Hao Dong
          </p>Computer Vision and Pattern Recognition (<b><font color="DarkRed">CVPR</font></b>), 2024 <br /><p></p>
            <!-- <b><font color="Green">[Poster]</font></b>&nbsp; -->
            <a href='https://openaccess.thecvf.com/content/CVPR2024/papers/Li_ManipLLM_Embodied_Multimodal_Large_Language_Model_for_Object-Centric_Robotic_Manipulation_CVPR_2024_paper.pdf'>[PDF]</a>&nbsp;
            <a href='https://sites.google.com/view/manipllm'>[Web page]</a>&nbsp;
        </div>
        <div class="spanner"></div>
      </div>

      <!-- CVPR 2024 -->
      <div class="paper" id="cvpr24_3"><img class="paper" src="papers/CVPR2024/notime.png" />
        <div>
          <p><a><b>No Time to Train: Empowering Non-Parametric Networks for Few-shot 3D Scene Segmentation</b></a><br />
            Xiangyang Zhu, Renrui Zhang, Bowei He, Ziyu Guo, <u><b>Jiaming Liu</b></u>, Han Xiao, Chaoyou Fu, Hao Dong, Peng Gao
          </p>Computer Vision and Pattern Recognition (<b><font color="DarkRed">CVPR Spotlight</font></b>), 2024 <br /><p></p>
            <!-- <b><font color="Green">[Poster]</font></b>&nbsp; -->
            <a href='https://openaccess.thecvf.com/content/CVPR2024/papers/Zhu_No_Time_to_Train_Empowering_Non-Parametric_Networks_for_Few-shot_3D_CVPR_2024_paper.pdf'>[PDF]</a>&nbsp;
            <!-- <a href='https://sites.google.com/view/manipllm'>[Web page]</a>&nbsp; -->
        </div>
        <div class="spanner"></div>
      </div>

      <!-- CVPR 2024 -->
      <div class="paper" id="cvpr24_4"><img class="paper" src="papers/CVPR2024/freekd.png" />
        <div>
          <p><a><b>FreeKD: Knowledge Distillation via Semantic Frequency Prompt</b></a><br />
            Yuan Zhang, Tao Huang, <u><b>Jiaming Liu</b></u>, Tao Jiang, Kuan Cheng, Shanghang Zhang
          </p>Computer Vision and Pattern Recognition (<b><font color="DarkRed">CVPR</font></b>), 2024 <br /><p></p>
            <!-- <b><font color="Green">[Poster]</font></b>&nbsp; -->
            <a href='https://openaccess.thecvf.com/content/CVPR2024/papers/Zhang_FreeKD_Knowledge_Distillation_via_Semantic_Frequency_Prompt_CVPR_2024_paper.pdf'>[PDF]</a>&nbsp;
            <!-- <a href='https://sites.google.com/view/manipllm'>[Web page]</a>&nbsp; -->
        </div>
        <div class="spanner"></div>
      </div>

      <!-- CVPR 2024 -->
      <div class="paper" id="cvpr24_5"><img class="paper" src="papers/CVPR2024/NTO.png" />
        <div>
          <p><a><b>NTO3D: Neural Target Object 3D Reconstruction with Segment Anything</b></a><br />
          Xiaobao Wei, Renrui Zhang, Jiarui Wu,  <u><b>Jiaming Liu</b></u>, Ming Lu, Yandong Guo, Shanghang Zhang
          </p>Computer Vision and Pattern Recognition (<b><font color="DarkRed">CVPR</font></b>), 2024 <br /><p></p>
            <!-- <b><font color="Green">[Poster]</font></b>&nbsp; -->
            <a href='https://openaccess.thecvf.com/content/CVPR2024/papers/Wei_NTO3D_Neural_Target_Object_3D_Reconstruction_with_Segment_Anything_CVPR_2024_paper.pdf'>[PDF]</a>&nbsp;
            <!-- <a href='https://sites.google.com/view/manipllm'>[Web page]</a>&nbsp; -->
        </div>
        <div class="spanner"></div>
      </div>

      <!-- AAAI 2024 -->
      <div class="paper" id="AAAI_2"><img class="paper" src="papers/AAAI2024/deweather-moe.png" />
        <div>
          <p><a><b>Efficient Deweather Mixture-of-Experts with Uncertainty-aware Feature-wise Linear Modulation</b></a><br />
            Rongyu Zhang, Yulin Luo, <u><b>Jiaming Liu</b></u>, Huanrui Yang, Zhen Dong, Denis Gudovskiy, Tomoyuki Okuno, Yohei Nakata, Kurt Keutzer, Yuan Du, Shanghang Zhang
          </p> Thirty-Eighth AAAI Conference on Artificial Intelligence <b><font color="DarkRed">AAAI</font></b>, 2024 <br /><p>
            <!-- <b><font color="Green">[Poster]</font></b>&nbsp; -->
            <a href='https://arxiv.org/pdf/2312.16610.pdf'>[PDF]</a>&nbsp;
            <!-- <a href='https://github.com/Neural-video-delivery/EMT-Pytorch-ECCV2022'>[Code]</a>&nbsp; -->
            <!-- <a href='papers/CVPRW2020/CVPR20_EDLCV_Oral_15min.pdf'>[Slides]</a>&nbsp; -->
        </div>
        <div class="spanner"></div>
      </div>

      <!-- AAAI 2024 -->
      <div class="paper" id="AAAI_1"><img class="paper" src="papers/AAAI2024/point-detr.png" />
        <div>
          <p><a><b>Leveraging Imagery Data with Spatial Point Prior for Weakly Semi-Supervised 3D Object Detection</b></a><br />
            Hongzhi Gao, Zheng Chen, Zehui Chen, Lin Chen, <u><b>Jiaming Liu</b></u>, Shanghang Zhang, Feng Zhao
          </p> Thirty-Eighth AAAI Conference on Artificial Intelligence <b><font color="DarkRed">AAAI</font></b>, 2024 <br /><p>
            <!-- <b><font color="Green">[Poster]</font></b>&nbsp; -->
            <a href=''>[PDF]</a>&nbsp;
            <!-- <a href='https://github.com/Neural-video-delivery/EMT-Pytorch-ECCV2022'>[Code]</a>&nbsp; -->
            <!-- <a href='papers/CVPRW2020/CVPR20_EDLCV_Oral_15min.pdf'>[Slides]</a>&nbsp; -->
        </div>
        <div class="spanner"></div>
      </div>
      
      <!-- MICCAI2023 -->
      <div class="paper" id="MICCAI2023_1"><img class="paper" src="papers/MICCAI2023/DIFF.png" />
        <div>
          <p><a><b>DiffuseIR:Diffusion Models For Isotropic Reconstruction of 3D Microscopic Images</b></a><br />
            Mingjie Pan, Yulu Gan, <u><b>Jiaming Liu</b></u>, Aimin Wang, Shanghang Zhang, Dawei Li
          </p>Medical Image Computing and Computer-Assisted Intervention (<b><font color="DarkRed">MICCAI</font></b>), 2023, CCF B <br /><p></p>
            <!-- <b><font color="Green">[Poster]</font></b>&nbsp; -->
            <a href='https://arxiv.org/abs/2306.12109'>[PDF]</a>&nbsp;
            <!-- <a href='https://opendrivelab.com/AD23Challenge.html#3d_occupancy_prediction'>[Challenge Web]</a>&nbsp; -->
            <!-- <a href='papers/CVPRW2020/CVPR20_EDLCV_Oral_15min.pdf'>[Slides]</a>&nbsp; -->
        </div>
        <div class="spanner"></div>
      </div>

      <div class="paper" id="CVPR2023_2"><img class="paper" src="papers/CVPR2023/CABM.png" />
        <div>
          <p><a><b>CABM: Content-Aware Bit Mapping for Single Image Super-Resolution Network With Large Input</b></a><br />
            Senmao Tian, Ming Lu, <u><b>Jiaming Liu</b></u>, Yandong Guo, Yurong Chen, Shunli Zhang
          </p>Computer Vision and Pattern Recognition (<b><font color="DarkRed">CVPR</font></b>), 2023 <br /><p>
            <!-- <b><font color="Green">[Poster]</font></b>&nbsp; -->
            <a href='https://openaccess.thecvf.com/content/CVPR2023/html/Tian_CABM_Content-Aware_Bit_Mapping_for_Single_Image_Super-Resolution_Network_With_CVPR_2023_paper.html'>[PDF]</a>&nbsp;
            <!-- <a href='https://github.com/Neural-video-delivery/EMT-Pytorch-ECCV2022'>[Code]</a>&nbsp; -->
            <!-- <a href='papers/CVPRW2020/CVPR20_EDLCV_Oral_15min.pdf'>[Slides]</a>&nbsp; -->
        </div>
        <div class="spanner"></div>
      </div>

      <div class="paper" id="CVPR2023_1"><img class="paper" src="papers/CVPR2023/Cloud-Device.png" />
        <div>
          <p><a><b>Cloud-Device Collaborative Adaptation to Continual Changing Environments in the Real-World</b></a><br />
            Yulu Gan, Mingjie Pan, Rongyu Zhang, Zijian Ling, <u><b>Jiaming Liu</b></u>, Shanghang Zhang
          </p>Computer Vision and Pattern Recognition (<b><font color="DarkRed">CVPR</font></b>), 2023 <br /><p>
            <!-- <b><font color="Green">[Poster]</font></b>&nbsp; -->
            <a href='https://openaccess.thecvf.com/content/CVPR2023/html/Pan_Cloud-Device_Collaborative_Adaptation_to_Continual_Changing_Environments_in_the_Real-World_CVPR_2023_paper.html'>[PDF]</a>&nbsp;
            <!-- <a href='https://github.com/Neural-video-delivery/EMT-Pytorch-ECCV2022'>[Code]</a>&nbsp; -->
            <!-- <a href='papers/CVPRW2020/CVPR20_EDLCV_Oral_15min.pdf'>[Slides]</a>&nbsp; -->
        </div>
        <div class="spanner"></div>
      </div>

       <!-- ECCV2022 -->
       <div class="paper" id="ECCV2022_3"><img class="paper" src="papers/ECCV2022/ECCV2022_3.png" />
        <div>
          <p><a><b>MTTrans: Cross-Domain Object Detection with Mean Teacher Transformer</b></a><br />
            Jinze Yu, <u><b>Jiaming Liu</b></u>, Xiaobao Wei, Haoyi Zhou, Yohei Nakata, Denis Gudovskiy, Tomoyuki Okuno, Jianxin Li, Kurt Keutzer, Shanghang Zhang
          </p>European Conference on Computer Vision (<b><font color="DarkRed">ECCV</font></b>), 2022 <br /><p>
            <!-- <b><font color="Green">[Poster]</font></b>&nbsp; -->
            <a href='https://arxiv.org/abs/2205.01643'>[PDF]</a>&nbsp;
            <!-- <a href='https://github.com/Neural-video-delivery/EMT-Pytorch-ECCV2022'>[Code]</a>&nbsp; -->
            <!-- <a href='papers/CVPRW2020/CVPR20_EDLCV_Oral_15min.pdf'>[Slides]</a>&nbsp; -->
        </div>
        <div class="spanner"></div>
      </div>

      <!-- BMVC2021 -->
      <div class="paper" id="bmvc2021"><img class="paper" src="papers/BMVC2021/BMVC2021.png" />
        <div>
          <p><a><b>SamplingAug: On the Importance of Patch Sampling Augmentation for Single Image Super-Resolution</b></a><br />
            Shizun Wang, Ming Lu, Kaixin Chen, <u><b>Jiaming Liu</b></u>, Xiaoqi Li, Ming Wu
          </p>The British Machine Vision Conference (<b><font color="DarkRed">BMVC</font></b>), 2021 <br /><p>
            <!-- <b><font color="Green">[Poster]</font></b>&nbsp; -->
            <a href='https://arxiv.org/abs/2111.15185'>[PDF]</a>&nbsp;
            <a href='https://github.com/littlepure2333/SamplingAug'>[Code]</a>&nbsp;
            <!-- <a href='papers/CVPRW2020/CVPR20_EDLCV_Oral_15min.pdf'>[Slides]</a>&nbsp; -->
        </div>
        <div class="spanner"></div>
      </div>

  <a name="Contest"></a>
    <div style="clear: both;">
    <div class="section">
    <h2 id="confpapers">Awards</h2>


    <!-- ICCV2023 -->
    <div class="paper" id="ICCV2023_C1"><img class="paper" src="papers/ICCV2023/ICCV-ctta.png" />
      <div>
        <p><a><b>1st Workshop on Visual Continual Learning, @ICCV 2023</b></a><br />
          <u><b>Jiaming Liu (Peking University)</b></u>, Ran Xu, Senqiao Yang, Peidong, Jia, Jiayi Ni
        </p><b><font color="DarkRed">Champion of ICCV</font></b> 2023 Challenge - Continual Test-time Adaptation for Semantic Segmentation <br /><p>
          <!-- <b><font color="Green">[Poster]</font></b>&nbsp; -->
          <!-- <a href='https://arxiv.org/pdf/2306.09117.pdf'>[Technical Report]</a>&nbsp; -->
          <a href='https://wvcl.vis.xyz/challenges#results'>[Challenge Web]</a>&nbsp;
          <!-- <a href='papers/CVPRW2020/CVPR20_EDLCV_Oral_15min.pdf'>[Slides]</a>&nbsp; -->
      </div>
      <div class="spanner"></div>
    </div>
    
    <!-- ICCV2023 -->
    <div class="paper" id="ICCV2023_C0"><img class="paper" src="papers/ICCV2023/ICCV-multi.png" />
      <div>
        <p><a><b>1st Workshop on Visual Continual Learning, @ICCV 2023</b></a><br />
          Zehui Chen (USTC), <u><b>Jiaming Liu (Peking University)</b></u>
        </p><b><font color="DarkRed">Champion of ICCV</font></b> 2023 Challenge - Multitask Learning for Robustness <br /><p>
          <!-- <b><font color="Green">[Poster]</font></b>&nbsp; -->
          <!-- <a href='https://arxiv.org/pdf/2306.09117.pdf'>[Technical Report]</a>&nbsp; -->
          <a href='https://wvcl.vis.xyz/challenges#results'>[Challenge Web]</a>&nbsp;
          <!-- <a href='papers/CVPRW2020/CVPR20_EDLCV_Oral_15min.pdf'>[Slides]</a>&nbsp; -->
      </div>
      <div class="spanner"></div>
    </div>


    <!-- CVPR2023 -->
    <div class="paper" id="CVPR2023_C"><img class="paper" src="papers/CVPR2023/OCC.png" />
      <div>
        <p><a><b>UniOcc: Unifying Vision-Centric 3D Occupancy Prediction with Geometric and Semantic Rendering</b></a><br />
          Mingjie Pan (Xiaomi Car), <u><b>Jiaming Liu (Peking University)</b></u>
        </p><b><font color="DarkRed">3rd place of CVPR</font></b> 2023 Challenge - Vision-Centric 3D Occupancy Prediction <br /><p>
          <!-- <b><font color="Green">[Poster]</font></b>&nbsp; -->
          <a href='https://arxiv.org/pdf/2306.09117.pdf'>[Technical Report]</a>&nbsp;
          <a href='https://opendrivelab.com/AD23Challenge.html#3d_occupancy_prediction'>[Challenge Web]</a>&nbsp;
          <!-- <a href='papers/CVPRW2020/CVPR20_EDLCV_Oral_15min.pdf'>[Slides]</a>&nbsp; -->
      </div>
      <div class="spanner"></div>
    </div>


      <div style="clear:both;">
        <p align="right">
          <font size="5">Last Updated on 21th February, 2024</a></font>
        </p>
      </div>

      <!-- <hr>
      <div id="clustrmaps-widget"></div>
      <script type="text/javascript" id="clustrmaps"
        src="//cdn.clustrmaps.com/map_v2.js?cl=ffffff&w=300&t=tt&d=3SbcvEtmc7QY9AEk0QS8aX5NjWpb6rfNMsOGTchiXzs"></script> -->

</body>

</html>